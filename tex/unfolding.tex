\chapter{Unfolding}
\label{sec:unfolding}

In order to allow a direct comparison between the experimental results
and the theoretical predictions, the parton level asymmetry spectra need
to be estimated.
As discussed in Sec.~\ref{sec:reconstruction} the asymmetry computed
from the reconstructed kinematics of the \ttbar{} system is affected
by the efficiency of correctly reconstructing the \dy{} sign. The
distortion induced on the \ac{} values depends on the kinematic region
considered and it can dilute the asymmetry up to half of its original
({\it true}) value.
The \ttbar{} MC simulation is used to map the distortions,
bin--dependent {\it efficiencies} and {\it migrations}, caused by
acceptance and resolution effects. The unfolding procedure consists in
combining this information with the distributions observed in data in
order to estimate the parton level distributions.

\section{Unfolding in particle physics}

The problem of de--convoluting  resolution effects in measured
distributions has been studied for a long time in particle physics,
with the first publications on the topic dating back to the
1980's~\cite{Blobel:157405}. An introduction to unfolding with a
review of the most commonly used techniques can be found
in~\cite{Cowan:2002in} and is summarized in this section. 

\subsection{The mathematical problem}

The general problem is determining the distribution $f(y)$ of a
stochastic variable using a sample of data $y_1,\cdots{},y_N$.
The problem arise from the fact that the measured values $x$ of the
observable $y$ are affected by random fluctuations due to measurement
errors. Therefore, each observation is characterized by a true value
$y$ and a measured value $x$, and the distributions of $x$ and $y$ are
related by the convolution
\begin{equation}
\label{eq:convolution}
f_{meas}(x) = \int~\mathcal{R}(x|y)~f_{true}(y)~dy,
\end{equation}
where $\mathcal{R}(x|y)$ is the response function.

Since no parametrization is available to describe the {\it true}
distribution $f_{true}(y)$, the corresponding {\it true} histogram
$\Truth{}=\{t_1,\cdots{},t_{N_t}\}$ with $N_t$ bins is
considered. Likewise, the {\it measured} distribution $f_{meas}(x)$ is
replaced by the {\it reconstructed} histogram $\Reco{}=\{r_1,\cdots{},r_{N_r}\}$ with
$N_r$ bins, and the response function becomes the $N_t\times N_r$ {\it
  response matrix} \TrasfMatrix{}.
The $t_i$ and $r_i$ values represent the expectation values of the number of
entries in bin $i$ of the true and reconstructed histograms,
respectively, for which Eq.~\ref{eq:convolution} becomes the matrix product
\begin{equation}
\label{eq:matrixprod}
\Reco{} = \TrasfMatrix{}\Truth{}.
\end{equation}

The goal of unfolding is to construct estimators for the $N_t$
parameters $\Truth{}=\{t_1,\cdots{},t_{N_t}\}$, given a distribution
\Reco{} of measured values. The response matrix \TrasfMatrix{} is derived from
an arbitrary reference sample, typically derived from simulation.
In order to derive \Truth{} from Eq.~\ref{eq:matrixprod}, the response
matrix \TrasfMatrix{} has to be inverted. However, depending on the
problem at hand, the response matrix might not be
invertible. Unfolding algorithms typically adopt approximations in
order to ensure invertibility. In addition, the estimator obtained from
matrix inversion is unbiased, and, therefore, with maximal
variance. Unfolding methods implement {\it regularization} procedures
that allow for more precise estimators at expense of introducing a bias.

\subsection{Unfolding algorithms}

Several unfolding techniques have been proposed and used in particle
physics. In this section, the most commonly used algorithms are
presented.

\paragraph{Bin-by-bin correction}
A simple method to overcome the invertibility problem consists on
constructing estimators for \Truth{} based on multiplicative
correction factors from MC simulation. The estimator for bin $i$ is
defined as
\begin{equation}
t_i = C_i~r_i,~{\rm with}~C_i=\frac{t_i^{MC}}{r_i^{MC}}.
\end{equation}
It is shown that this estimator, neglecting to model off--diagonal
migrations, is strongly biased towards the simulated model used to
derive the correction factors. 
\paragraph{Singular value decomposition}
In this method~\cite{Hocker:1995kb}, the inverted response matrix is
obtained by singular value decomposition. The $N_t\times N_r$ matrix
\TrasfMatrix{} is factorized as 
\begin{equation}
\TrasfMatrix{} = U~S~V^T,
\end{equation}
where $U$ is an $N_t\times N_t$ unitary matrix, $V$ is an $N_r\times
N_r$ unitary matrix, and $S$ is an $N_t\times N_r$ diagonal matrix
with non-negative eigenvalues $s_i$.
The matrices $U$ and $V$ are used to reduce Eq.~\ref{eq:matrixprod}
to a diagonal system of equations
\begin{equation}
\label{eq:matrixprod}
\vect{d} = S~\vect{z},
\end{equation}
where $\vect{d}=U^T\Reco{}$ and $\vect{z}=V^T\Truth{}$. Thus, the
problem is reduced to the trivial inversion of a diagonal matrix.
However, the authors of the method show that, for very small
eigenvalues $s_i$, statistical fluctuations in \Reco{} are amplified giving
meaningless solutions for \Truth{}. The problem is addressed by
introducing a regularization that stabilizes the result by suppressing
non--physical fluctuations. In particular, the method implements a
Tikhonov regularization~\cite{Tikhonov1943stability}, which suppresses
results with high curvature (see Sec.~\ref{sec:fbuprior}).
\paragraph{Iterative bayesian unfolding}
A different approach to unfolding is taken in this
method~\cite{dagostini1995NIMPA}, where the estimators for \Truth{}
are constructed using Bayes' theorem:
\begin{equation}
t_i = N~p(t_i|\Reco{}) = N~\sum_j~\frac{p(r_j|t_i)~p(t_i)}{p(r_j)},
\end{equation} 
where the conditional probability $p(r_i|t_i)$ corresponds to the
elements of the response matrix \TrasfMatrix{}, $p(r_j)$ is the
fraction of events in bin $j$ of the measured distribution \Reco{},
and $p(t_i)$ is the prior assumption on the true bin content to be
estimated. At each iteration, $p(t_i)$ is updated with the estimated
$t_i$ value. The bias is introduced by the choice of the starting
value for $p(t_i)$, while at each iteration the bias is reduced, at
expense of increasing the variance.

\section{Fully Bayesian Unfolding}
\label{sec:fbu}
The unfolding methods presented so far try to cope with the difficulty
of solving the inverse problem by introducing a bias. The algorithm
studied and implemented for this thesis provides an exact solution to
the unfolding problem by removing the need of inverting the response
matrix.

The Fully Bayesian Unfolding~\cite{fbu} consists in the application of
Bayesian inference to the problem of unfolding. This application can
be stated in the following terms: given an observed spectrum
$\Data\in\Integer^{N_r}$ and a response matrix
$\TrasfMatrix\in\Real{}^{N_r}\times{}\Real{}^{N_t}$, the posterior
probability of the true spectrum $\Truth{}\in{}\Real{}^{N_t}$ follows
the probability density
\begin{equation}
\conditionalProb{\Truth{}}{\Data{},\TrasfMatrix{}}
\propto{}
\conditionalLhood{\Data{}}{\Truth{},\TrasfMatrix{}}
\cdot{}
\pi{}\left(\Truth{}\right)
\end{equation}
where \conditionalLhood{\Data{}}{\Truth{},\TrasfMatrix{}} is the
conditional likelihood for \Data{} given \Truth{} and \TrasfMatrix{},
and $\pi{}$ is the prior probability density for the true spectrum
\Truth{}.
Each of these ingredients is described in the following sections.

\subsection{Likelihood}
\label{sec:fbullhood}
Under the assumption that the data are poissonian counts, the
likelihood \conditionalLhood{\Data{}}{\Truth{},\TrasfMatrix{}} can be
computed from the following two pieces of information, contained in
the response matrix \TrasfMatrix{}:
\begin{itemize}
\item the probability $P(r|t)$ of an event to be
  produced in the true bin $t$ and to be observed in the reconstructed
  bin $r$;
\item the efficiency $\epsilon{}_t$ for an event produced in the
  true bin $t$ to be reconstructed in any bin $r$.
\end{itemize}
The above quantities allow the extrapolation of the reconstructed spectrum
\Reco{} corresponding to a given true spectrum \Truth{} as in
\begin{equation}
r_i(\Truth{},\TrasfMatrix{}) = \sum_{j=0}^{N_r}\epsilon_j\cdot{}p(r_i|t_j).
\end{equation}
The likelihood is then defined by comparing the observed spectrum
\Data{} with the expected one, which includes the background
prediction \Bckg{}:
\begin{equation}
\conditionalLhood{\Data{}}{\Truth{}} =
\prod_{i=1}^{N_r}Poisson(d_i,r_i(\Truth{},\TrasfMatrix{})+b_i).
\end{equation}

\subsection{Prior}
\label{sec:fbuprior}
While the response matrix can be estimated from the simulated
sample of signal events, the prior probability density \prior{} is to
be chosen according to what we know about \Truth{} before the
measurement is performed.
The simplest choice is an {\it uninformative}
prior that assigns equal probabilities to all \Truth{} spectra within
a wide range $\left[\Tinf{}, \Tsup{}\right]$\footnote{In the analysis,
the range $[\tilde{\Truth{}}/5, 2\tilde{\Truth}]$ is considered, where
$\tilde{\Truth{}}$ is the true spectrum in simulation.}:
\begin{equation}
\prior{}
\propto{}
\begin{cases}
1 & \text{if }
T_{t}\in{}\left[\Tinf{}, \Tsup{}\right], \forall{}t\in{}\left[1, N_t\right] \\
0 & \text{otherwise} \\
\end{cases}
\end{equation}
A more general definition for the prior is given by 
\begin{equation}
\prior{}
\propto{}
\begin{cases}
e^{\alpha{}S(\Truth{})} & \text{if }
T_{t}\in{}\left[\Tinf{}, \Tsup{}\right], \forall{}t\in{}\left[1, N_t\right] \\
0 & \text{otherwise} \\
\end{cases}
\end{equation}
where $\alpha{}$ is an arbitrary strength parameter, and
$S(\Truth{})$ is a regularization function.
The choice of $\alpha$ determines the impact of the prior on
\conditionalProb{\Truth{}}{\Data{}}, while $S(\Truth{})$ determines
which additional information is used to constrain the parameter
space.

In the analysis of the dataset at \seventev{}, a curvature prior is
used for the unfolding of the inclusive \ac{} and of the
differential \ac{} spectrum as a function of \pttt{}.
In the curvature prior the regularization function $S(\Truth{})$ is
the difference between the \emph{Thikonov} regularization functions
$C(\Truth)$ (i.e. curvatures) for the given point \Truth{} and for the
truth distribution in simulation. 
The \emph{Thikonov} regularization function is defined as
\begin{equation}
        C(\Truth{}) =
        \sum_{t=2}^{N_t-1}\left[(T_{t+1}-T_{t})-(T_t-T_{t-1})\right]^2.
\end{equation}
The application of this prior disfavors, with strength $\alpha{}$,
\Truth{} candidates with curvature that is very different from
the one of the truth spectrum in simulation.
A regularization strength $\alpha{}=-10^{-8}$ is used, reducing the
uncertainty on the inclusive \ac{} by 50\%, with negligible bias.

\subsection{Sampling}
\label{sec:sampling}
Having chosen the prior, the posterior probability density
\conditionalProb{\Truth{}}{\Data{}} is determined by sampling the
$N_t$--dimensional parameter space, and evaluating for each point
\conditionalLhood{\Data{}}{\Truth{}} and \prior{}, thus performing a
numerical integration. For a large number of parameters, direct
sampling techniques become extremely inefficient; therefore, the
Metropolis--Hastings algorithm, a Markov
Chain Monte Carlo (MCMC) method specialized for multi--dimensional
sampling, is used in FBU.
Combining this set of points with the weight given by
$\conditionalLhood{\Data{}}{\Truth{},\TrasfMatrix{}}\cdot{}\pi{}$, one
can determine not only the posterior probability density distribution
for each bin of the spectrum, but also the posterior probability
density distribution for any quantity that is computed from the
spectrum, such as \ac{}:
\begin{equation}
\conditionalProb{\ac{}}{\Data{}} = 
\int~
\ac{}(\Truth{})~
\conditionalProb{\Truth{}}{\Data{}}~d\Truth{}
\end{equation}
The mean and RMS of the posterior probability
density for \ac{} are taken as central value and statistical uncertainty respectively.
As an example, the unfolding input \dy{} distributions in data and
background are shown in Fig.~\ref{fig:unfinput}, as well as the
response matrix.
\begin{figure}[!htb]\centering
  \includegraphics[width=0.495\textwidth]{figures/unfolding/unfInputProtosA2pos__lin}
  \includegraphics[width=0.495\textwidth]{figures/unfolding/unfOutputProtosA2pos__lin}
  \caption{
    \label{fig:unfinput}
   Unfolding input histograms for data and backgrounds (left) and
   response matrix (right) for the inclusive \ac{} measurement at \eighttev{}.
   }
\end{figure}
%
The posterior probability densities for the \dy{} distribution and the corresponding
\ac{} are illustrated in Fig.~\ref{fig:posteriorIncl}.
%
\begin{figure}[!htb]\centering
  \includegraphics[width=0.495\textwidth]{figures/unfolding/exampleBinPosterior}
  \includegraphics[width=0.495\textwidth]{figures/unfolding/example_inclusive_posterior}
  \caption{
    \label{fig:posteriorIncl}
   Posterior probability densities for one bin of the \dy{}
   distribution (left) and for the asymmetry \ac{} (right), as
   obtained in the \ac{} measurements at \eighttev{}.
   }
\end{figure}

\subsection{Binning choice and bias}

The choice of the binning for the \dy{} distribution is driven by two
factors:
\begin{itemize}
\item The number of parameters to estimate (the \dy{} bin yields)
  affects their variance; with fewer bins, the relative statistical
  errors on the bin contents are reduced. Therefore the resulting
  statistical error on \ac{} is smaller with fewer bins. At least two
  bins are necessary to compute \ac{} (positive and negative side of
  the \dy{} distribution).
\item A large number of bins allows an accurate mapping of the
  migrations, yielding unbiased estimates for each bin
  content. However, only migrations that change the \dy{} sign affect
  the computation of \ac{}. Such migrations are more likely for small
  \dy{} values, therefore a fine binning of the central \dy{} region
  ensures an unbiased measurement of \ac{}. 
\end{itemize}
The benchmarks driving the choice of the binning are thus the
statistical error and the bias on \ac{}.

The statistical error on \ac{} is validated by performing unfolding in
an ensemble of pseudo-experiments where statistically independent
pseudo-data distributions are generated based on Poisson statistics.  
The distribution of $(\ac{}^{unf}-\ac{}^{true})/\sigma^{unf}$
({\it pull}) in the ensemble is considered, where $\ac{}^{unf}$ and
$\sigma^{unf}$ are the unfolded asymmetry value and its uncertainty,
while $\ac{}^{true}$ is the parton level asymmetry of the sample used
to generate the pseudo-data. As shown in Fig.~\ref{fig:pulllinearity}, the
RMS of the pull distribution is $\approx{}1$, indicating that the
uncertainty is correctly estimated.  

The bias in the unfolding response is measured by studying the
unfolded asymmetry in pseudo-data samples for which the true
asymmetry is known. The asymmetric samples are built by reweighing the
baseline \ttbar{} simulation to the parton level \dy{} spectrum of BSM axi--gluon
samples (see Sec.~\ref{sec:theory}) corresponding to $\pm2\%$, $\pm4\%$
and $\pm6\%$ asymmetries. In order to minimize the effect of
statistical fluctuations, the unfolding procedure for each reference
point is repeated in $N_{PE}$ pseudo-experiments. For each reference
point with true asymmetry $\ac{}^{true}$, the mean of the unfolded
values $\ac{}^{unf}$, with error $\sigma/\sqrt{N_{PE}}$, is then
obtained. The set of ($\ac{}^{true}$,$\ac{}^{unf}$) pairs is
interpolated with a straight line parametrized as: 
\begin{equation}
\ac{}^{unf}=a\cdot{}\ac{}^{true}+b,
\end{equation}
where $a$ and $b$ are the {\it slope} and {\it offset} parameters.
The unfolding response is considered unbiased when the distance
$|1-a|$ and the offset $b$ are smaller than 10\% of the relative and
absolute statistical error, respectively.
For all measurements at \seventev{} and \eighttev{}, four is the
minimum number of bins which allows an unbiased
response. Figure~\ref{fig:pulllinearity} shows the linearity test for
the inclusive \ac{} measurement at \eighttev{}.

\begin{figure}[!htb]\centering
  \includegraphics[width=0.495\textwidth]{figures/unfolding/pulls_bin0__A2pos_combined_10m10}
  \includegraphics[width=0.495\textwidth]{figures/unfolding/linearity_inclusive_bin0_0tagex1tagex2taginQ_10m10}
  \caption{Pull distribution (left) and linearity test (right) for the
    inclusive \ac{} measurement at \eighttev{}}
  \label{fig:pulllinearity}
\end{figure}

\section{Marginalization}
\label{sec:marginalization}

The treatment of systematic uncertainties is naturally included in the
Bayesian inference approach by extending the likelihood
\conditionalLhood{\Data{}}{\Truth{}} with nuisance parameters terms.
The {\it marginal} likelihood is defined as
\begin{equation}
  \conditionalLhood{\Data{}}{\Truth{}}=
  \int
  \conditionalLhood{\Data{}}{\Truth{},\thetavec{}} 
  \cdot{} \pi{}(\thetavec{})
  ~d\thetavec{},
\end{equation}
where \thetavec{} are the nuisance parameters, and
$\pi{}(\thetavec{})$ their prior probability densities, which are
assumed to be Gaussian distributions $G$ with $\mu=0$ and $\sigma=1$.

A nuisance parameter is associated to each of the uncertainty sources
listed in Sec.~\ref{sec:syst_objects}. Two categories of nuisances
are considered: the normalizations of the background processes
($\thetavec{}_b$), and the uncertainties associated to the objects
identification, reconstruction and calibration ($\thetavec{}_s$).
While the first ones only affect the background predictions, the
latter, referred to as object systematics, affect both the
reconstructed distribution for \ttbar{} signal and for
the total background prediction, referred to as
$\Reco{}(\Truth{};\thetavec{}_s)$ and
$\Bckg{}(\thetavec{}_s,\thetavec{}_b)$, respectively.
The marginal likelihood becomes then
\begin{equation}
\label{eq:marginalLhood}
  \conditionalLhood{\Data{}}{\Truth{}}=
  \int
  \conditionalLhood{\Data{}}{\Reco{}(\Truth{};\thetavec{}_s),\Bckg{}(\thetavec{}_s,\thetavec{}_b)} 
  \cdot{} G(\thetavec{}_s) ~G(\thetavec{}_b)
  ~d\thetavec{}_s ~d\thetavec{}_b.
\end{equation}

The marginal posterior probability density for \Truth{} is computed by
sampling the $N_t+N_{np}$ parameter space, where $N_{np}$ is the
total number of nuisance parameters, and projecting the sample over
the \Truth{} parameter space. The projections over each nuisance
parameter gives the corresponding posterior probability density,
which matches the Gaussian prior for unconstrained nuisance
parameters, while it has a narrower shape for nuisance parameters
that can be measured in the dataset (see Fig.~\ref{fig:nuispar}).
The posterior probability density for \ac{} is computed as described in
Sec.~\ref{sec:sampling} with the difference that the RMS of the
marginal posterior represents the total uncertainty. Analogously, each
nuisance parameter is estimated by the mean and RMS of the
corresponding projection of the posterior probability density.

\begin{figure}[!htb]\centering
  \includegraphics[width=0.495\textwidth]{figures/unfolding/unconstrainednp}
  \includegraphics[width=0.495\textwidth]{figures/unfolding/constrainednp}
  \caption{Prior and posterior distributions for nuisance parameters
    corresponding to a component of the JES uncertainty (left) and of
    the b-tagging efficiency (right) in the \eighttev{} measurement.}
  \label{fig:nuispar}
\end{figure}

\subsection{Channel combination}

As discussed in Sec.~\ref{sec:wjets}, the combination of
orthogonal channels with different background compositions is crucial
to estimate precisely the \wjets{} contamination in the \eighttev{}
data sample.
The marginalization approach provides a natural framework to treat
simultaneously unfolding and background estimation using
multiple data regions. Given the distributions $\Data{}_i$ measured in $N_{ch}$
independent channels, the likelihood definition~\ref{eq:marginalLhood}
is extended to the product  
\begin{equation}
  \conditionalLhood{\{\Data{}_1\cdots{}\Data{}_{N_{ch}}\}}{\Truth{}}=
  \int
  \prod_{i=1}^{N_{ch}}\conditionalLhood{\Data{}_i}{\Truth{};\thetavec{}} 
  \cdot{} G(\thetavec{})
  ~d\thetavec{},
\end{equation}
where the nuisance parameters are evaluated simultaneously for all the
factors.

The \eighttev{} measurements are
performed using a combination of six channels based on
the lepton charge and the $b$--jet multiplicity. In addition to the
expected number of \ttbar{} events for each bin in \Truth{}, the
\wjets{} calibration factors $K_{\bbbar{}/\ccbar{}}$, $K_c$ and $K_{light}$ are free
parameters in the likelihood.
The posterior probability density is thus
\begin{equation}
\begin{split}
  \conditionalProb{\Truth{}}{\{\Data{}_1\cdots{}\Data{}_{N_{ch}}\}}=&
  \int
  \prod_{i=1}^{N_{ch}}\conditionalLhood{\Data{}_i}{\Reco{}_i(\Truth{};\thetavec{}_s),\Bckg{}_i(\thetavec{}_s,\thetavec{}_b)}  \\
&  ~G(\thetavec{}_s)
  ~G(\thetavec{}_b)
  ~\pi{}(\Truth{})
  ~\pi{}(K_{\bbbar{}/\ccbar{}})
  ~\pi{}(K_c)
  ~\pi{}(K_{light})
  ~d\thetavec{}_s
  ~d\thetavec{}_b,\\
\end{split}
\end{equation}
with
$\Bckg{}=\Bckg{}(K_{\bbbar{}/\ccbar{}},K_c,K_{light};\thetavec_s,\thetavec{}_b)$,
and the $\pi{}$'s are uninformative priors.
Therefore, normalization nuisance parameters are considered
only for the remaining background processes: single top, \zjets{} and
multijet. An independent nuisance parameters is assigned to \zjets{}
and multijet normalizations for each $b$--jet multiplicity, given that
the modeling of $b$--tagging efficiencies is not calibrated
specifically for these processes. Independent nuisance parameters are
considered for the multijet background estimation in \mujets{} and in
\ejets{}. An additional nuisance parameters is
assigned to the simulated single top and \zjets{} backgrounds,
corresponding to uncertainty on the integrated luminosity to which the
MC sample are normalized.

A total of eleven normalization nuisance parameters $\thetavec_b$ is
considered, while the object definition parameters $\thetavec_s$ are 53. 
Table~\ref{tab:nuispar} summarizes the expected precision and measured
values for all nuisance parameters. The expected precision is obtained
by evaluating the likelihood for a pseudo--data sample corresponding
to the sum of background and signal predictions; thus the central
value for each parameter is measured to be $\theta\sim0$, while the
uncertainty is $\delta\theta\leq1$, depending on the constraining power of
the dataset on each parameter.

\begin{table}
  \centering
  \begin{tabular}{lcc}
    \toprule
    parameter & expected $\theta\pm\delta\theta$ & measured $\theta\pm\delta\theta$ \\
    \midrule
    Luminosity & $0.0\pm1.0$& $0.0\pm1.0$\\
    TO BE FILLED & & \\
   \bottomrule
  \end{tabular}
  \caption{Expected and measured nuisance parameters.}
  \label{tab:nuispar}
\end{table}