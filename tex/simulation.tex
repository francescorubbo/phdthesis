\chapter{Event simulation}
\label{sec:simulation}

An accurate simulation of the Physics processes and of the interaction
of particles with the detector is necessary to model the impact of the
analysis procedure on the measured quantities, and to estimate the
background composition expected in data. A set of computer programs
known as Monte Carlo (MC) event generators simulate the physical
processes 

Pseudo-random numbers are used to simulate the event-by-event
fluctuations intrinsic of quantum processes. MC generators make use of
the factorization principle (Section~\ref{sec:factprinciple}), so the
different phases of the $pp$ collision are considered independently.

\section{Simulation of pp collisions}
\label{sec:MCsimulation}

A pp collision event in Monte Carlo simulation
is the combination of different sub-processes,
illustrated in Figure~\ref{fig:collision}.

The event is simulated in several steps. Two protons collide and
undergo a deep inelastic interaction, with a large momentum transfer.
The process of interest is generated by the {\it hard interaction} of
two partons within the protons and it is computed at a fixed order (LO
or NLO) in perturbation theory, as described in Section~\ref{sec:QCDtheory}. 
The lower energy interactions between the proton remnants,
referred to as {\it underlying event} UE, are described with
phenomenological models.

Since the partons involved in the hard interaction are color charged,
they can radiate gluons. Emission associated with the two colliding partons  
is referred to as {\it Initial State Radiation} (ISR), while {\it
  Final State Radiation} (FSR) is emitted by the partons produced in
the collision. The emitted gluons can emit further gluons or split in
quark/anti-quark pairs leading to the formation of {\it parton
  showers}.

The radiation process is effectively described by perturbative QCD until
the showers develop into processes at energy below
$\approx{}1~\GeV{}$. 
At this stage, {\it hadronization} takes place, where partons are
bound into colorless hadrons. Phenomenological models are used to
describe the decay of hadrons into the final state particles that
interact with the detector.

\subsection{Hard interaction}
\label{sec:hardinteraction}

[Brief reference to theory chapter where QCD theory is discussed.]

\subsection{Parton shower}
\label{sec:partonshower}

The parton showers represent higher-order corrections to the hard
interaction, corresponding to the production of additional partons.
Since radiative corrections at a fixed perturbative order are
divergent at low energies ({\it infrared divergence}) or small angles
({\it collinear divergence}), only the dominant contributions are
included iteratively.

There are three possible processes for QCD emission ({\it splitting}): 
$q\to gq$, $g\to gg$ and $g\to q\bar{q}$.
The cross section for each of these processes corresponds to the
product of the production cross section for the original parton and a
factor accounting for the splitting probability. Hence, for each
splitting process $i$, the $n+1$-parton differential cross section is
defined, at the LO, by

\begin{equation}
  d\sigma_{n+1}\approx{} d\sigma_{n}
  \frac{\alpha_S}{2\pi}\frac{d\theta^2}{\theta^2} dz d\phi P_i(z,\phi) 
 \label{eq:splitting}
\end{equation}

where $\theta$ and $\phi$ are the opening angle and azimuthal angle of
the splitting, and $P_i$ is the splitting function, which describes
the distribution of the fraction $z$ of energy of the original parton,
assigned to the new parton.
The simulation algorithm develops the shower by applying
Eq.~\ref{eq:splitting} iteratively, for each parton involved in the
hard interaction.

In order to define the starting and final stage of the evolution of
the parton shower, the {\it virtuality} $q^2$ of the parton undergoing
the splitting is defined as the invariant mass of the two partons
produced. The initial virtuality is required to be smaller than the
momentum transfer of the hard process, and the shower is terminated
when the virtuality has fallen below the hadronization scale
($q^2=Q_0^2\simeq1\GeV^2$).
The parton shower takes into account virtual emissions that are
reabsorbed in quantum loops as a probability of {\it not} splitting in
a given virtuality range $[q_1^2,q_2^2]$, referred to as {\it Sudakov form factor}

\begin{equation}
  \label{eq:sudakov} 
  \Delta_i (q_1^2, q_2^2) = \exp \left[ - \int_{q_2^2}^{q_1^2}
    \frac{dq^2}{q^2} \frac{\alpha_S}{2\pi}
    \int_{\frac{Q_0^2}{q^2}}^{1-\frac{Q_0^2}{q^2}} dz \int_0^{2\pi} d\phi{} P_{i} (z,\phi) \right]
  \end{equation}

The evolution of the parton shower is therefore governed by the
Sudakov factor. Given the initial scale $Q^2$, the MC generator solves
the equation $\Delta_i(Q^2, q_1^2)=R_1$, where $R_1$ is a random
number uniform on the interval [0,1], for the virtuality $q_1^2$ of
the first splitting. If the condition $q_1^2<Q_0^2$, the shower
development is terminated and hadronization takes place. Otherwise,
the procedure is repeated for each new parton produced by the
splitting, taking $q_1^2$ as initial scale.
For each splitting the variables $z$ and $\phi$ are generated
according to the distribution defined by the splitting function.

\subsubsection{ISR and FSR showers}
\label{sec:isrfsr}

The description above applies to the development of parton showers
associated with partons produced in the hard interaction, starting at
a high energy scale $Q^2$ and progressively reaching the hadronization
scale. This process is typical of FSR parton showers that are
generated from outgoing partons of the hard interaction.

In the case of ISR parton showers, the radiation is emitted by the
colliding partons, and there is an important difference in the shower
evolution, as the final energy of the showering is set by the hard
interaction energy scale.
MC generators implement a mechanism of {\it backward evolution} that
first sets the correct parton momentum fractions for the hard scatter,
and then develops the showers backward, with the intermediate partons
gaining energy at each emission. The Sudakov form factors are then
slightly different from Equation~\ref{eq:sudakov}, being rescaled by a
factor that takes into account the PDFs of the parton before and after
splitting.

\section{Generators}\label{sec:generators}

Monte Carlo generators can be classified as either {\it multi-purpose} generators,
i.e. capable of performing the full simulation chain described in this chapter,
or as {\it specialized} generators, i.e. devoted to a single aspect of the
simulation which they often describe with higher accuracy than
typical multi-purpose generators. 

We briefly present in the following sections the main characteristics of the generators
that are used in the analyses that are the subject of this dissertation.

\subsubsection*{PYTHIA}

\texttt{PYTHIA}~\cite{PYTHIA,Sjostrand:2007gs} is a multi-purpose Monte Carlo generator
using ME computed at LO for $2 \to n$ ($n\leq 3$) processes and PS with emissions
ordered in $p_T$ instead of angle. The Lund string model is used for hadronization
and UE simulation is included.
Minimum bias events with $p_T>\hat{\pt}_{min}$ are used to model interactions
between proton remnants.


\subsubsection*{HERWIG}

\texttt{HERWIG}~\cite{HERWIG} is a multi-purpose Monte Carlo generator
using ME computed at LO for $2 \to 2$ processes and PS with emissions ordered in angle. 
The cluster model is used for hadronization and for the UE description, \texttt{HERWIG}
is typically interfaced with the external package \texttt{JIMMY}~\cite{jimmy} which
simulates UE as MPIs in  $2 \to 2$ processes with the ME computed at LO.


\subsubsection*{ACERMC}

\texttt{ACERMC}~\cite{acermc} is a Monte Carlo generator computing
ME at LO and typically interfaced either with \texttt{PYTHIA} or 
\texttt{HERWIG} for the modeling of PS, hadronization and UE.


\subsubsection*{ALPGEN}

\texttt{ALPGEN}~\cite{ALPGEN} is a Monte Carlo generator specialized for
ME computation of $2 \to n$ ($n\leq 9$) events at LO, with cross sections 
computed using the ALPHA algorithm \cite{ALPGEN_0}. It is interfaced either
with \texttt{PYTHIA} or \texttt{HERWIG} for PS development 
and hadronization, and ME/PS matching
is done in the MLM scheme.
UE is simulated through \texttt{PYTHIA}. 

The various parton multiplicity samples are then normalized to their LO cross section
and combined into an inclusive sample, which is finally typically normalized 
to an inclusive cross section calculated at higher order in pQCD. 

\subsubsection*{MC@NLO}

\texttt{MC@NLO}~\cite{mcatnlo} is a Monte Carlo generator simulating ME at
NLO, where the use of 1-loop corrections introduce the possibility of having
negative weighted events. Theoretical uncertainties on the inclusive cross
section is reduced thanks to the use of full NLO corrections, but higher-multiplicity
parton emission beyond the first one is simulated through PS in \texttt{HERWIG} which has
a poorer description of hard emissions. 
Hadronization and UE are also simulated through \texttt{HERWIG}
(and hence \texttt{JIMMY}).

\subsubsection*{POWHEG}

\texttt{POWHEG}~\cite{powheg} is a Monte Carlo generator computing
ME at NLO and typically interfaced either with \texttt{PYTHIA} or 
\texttt{HERWIG} for the modeling of PS, hadronization and UE.
The choice made at the level of interfacing the ME with PS
can lead to large differences in the description of events with
 high jet multiplicity in the final state between \texttt{POWHEG} 
and \texttt{MC@NLO}.



\section{ATLAS detector and trigger simulation}\label{sec:MCdetector}

For direct comparison with data, events generated with Monte Carlo
simulation need to be processed through detector simulation~\cite{atlas_sim}.

The detector material, geometry and response
are modeled using the {\tt GEANT4}~\cite{geant} package.
The software

This code evolves the particles simulating a real environment
where they can cross volumes of material and leave energy
deposits or hits. Figure~\ref{fig:atlasdisplay} shows an
example of a Higgs boson decay event simulated in the detector
geometry.
The interaction of particles with ATLAS subsystems is converted
into detector signals of the same sort of the real read-out and
at this point the same kind of reconstruction algorithms shape
the detector output into physical objects. During test-beam periods
the \texttt{GEANT4} parameters have been tuned to best simulate
the response of the different 
ATLAS subsystems and the performance of detector simulation
has been extensively checked also with data calibrations.

%\begin{figure}[htb]\begin{center}
%	\subfigure{
%  	\includegraphics[width=0.8\textwidth]{montecarlo/figures/higgs_event_csc}}
%	\caption{Display of a high-$p_T$ $H \to ZZ*\to ee\mu\mu$ decay ($m_H$ = 130~GeV), 
%        after full simulation and reconstruction in the ATLAS detector. Picture from \url{http://www.atlas.ch}. \label{fig:atlasdisplay}}
%\end{center}\end{figure}


The detector simulation relies on the usage of two databases: the 
{\it geometry database} contains the information on the detector volumes
like dimensions, geometry, position and material composition, while
the {\it conditions database} is constantly updated with the information
on the real detector real-time conditions as dead channels, misalignments,
temperature. Since conditions vary from run to run, it is important that
the detector simulation reproduces as close as possible the real status
of ATLAS during a particular data period. Also for this reason, Monte
Carlo samples are regularly reproduced consistently with {\it data reprocessings}
or {\it data releases}.  For the analyses presented in this dissertation,
the Monte Carlo production tagged as \texttt{mc12} is used, performed
within release 17 of the ATLAS analysis framework \texttt{ATHENA}~\cite{Calafiura:2005zz}.



\section{Monte Carlo samples corrections}\label{sec:mcweights}

At the end of the full Monte Carlo simulation chain, after the detector simulation
and event reconstruction steps, the generated samples still need some corrections to
better model data. The main event reweighting is to correct Monte Carlo samples to
the correct theoretical cross section of the process and to the number of expected
data events, which comes from the luminosity measurements. As typical in Monte
Carlo techniques, an higher number of randomly produced events assures a better modeling
of the system under study, and therefore usually a very high number of events are
produced for each sample to maximize the confidence that all the relevant configurations
have been copiously simulated. The event weight $w$ to be applied (event by event) is defined
as:
\begin{equation}\label{eq:mcweight}
w = \dfrac{\sigma\times k}{N}\int \mathcal L dt,
\end{equation}
where $\sigma$ is the process theoretical cross section, $N$ is the number of Monte
Carlo events, $\int \mathcal L dt$ the integrated luminosity and $k$ is the so-called
$k-$factor, which is a correction to 
the LO cross section to reproduce a higher-order
(e.g. NLO) calculation.

In addition, a weight to account for pile-up effect is to 
be applied, to match the
expected number of interactions per bunch crossing $<\mu>$
 in real data-taking conditions. 
Additional correction are applied in the analyses
to ensure that the simulation reproduces the efficiency,
energy scale and resolution of different
physics objects in data, as will be described in
Chapter~\ref{chap:objects}.