\chapter{Analysis Strategy}
\label{sec:strategy}

This chapter introduces the strategies implemented to measure the
asymmetries in the datasets collected at \seventev{} and \eighttev{}.
Due to the different amount of data and data--taking conditions for
the two datasets (see Sec.~\ref{sec:experiment}), slightly different
procedures are adopted in order to obtain the best precision
achievable in both scenarios. The general strategy is illustrated in
Sec.~\ref{sec:genstrategy}, while the event
selection criteria and the signal and background modeling are
described in Sec.~\ref{sec:evtsel} and~\ref{sec:bckgmodel},
respectively, detailing the specific choices for each analysis. The
comparison between data and prediction is shown in
Sec.~\ref{sec:datamc}, and the sources of systematic uncertainty are
discussed in Sec.~\ref{sec:syst}.

\section{General strategy}
\label{sec:genstrategy}

The goal of the analysis is the measurement of the charge asymmetry
\ac{} integrated over the kinematics of the \ttbar{} system,
referred to as {\it inclusive \ac{}}, and its differential spectra as
a function of the invariant mass of the \ttbar{} pair \mtt{}, its
transverse momentum \pttt{}, and the absolute value of its rapidity
\ytt{}. In order to achieve this goal, three steps are required:
\begin{itemize}
\item Samples enriched in \ttbar{} events are selected and the
  background composition in the samples is estimated.
\item The kinematics of the \ttbar{} system is reconstructed in order
  to access the top quark and antiquark rapidities and compute the
  \dy{} observable described in Sec.~\ref{sec:topca}.
\item The information about the distortions introduced by the two
  above steps is used to estimate the parton--level asymmetries with
  an {\it unfolding} procedure.
\end{itemize}

While the same reconstruction and unfolding procedures, described in
the following chapters, are used for both datasets, the selection
strategy and the background estimation is driven by the amount of data
available. 
The dataset at \eighttev{} is approximately six times larger than the
one at \seventev{}, with two implications: on one hand the statistical
uncertainties are greatly reduced, making it crucial to control
sources of systematic uncertainties in order to perform precise
measurements; on the other hand, the large dataset can be split in
subsamples without significant loss in statistical power, and the
properties of the background can be exploited to determine
its composition. Therefore, the measurements at \seventev{} are
performed using a single sample enriched in \ttbar{} events, while the
measurements at \eighttev{} are performed using six subsamples with
different signal purity and background composition. The following sections
describe the details of the selection requirements applied to the
reconstructed objects in order to maximize the \ttbar{} signal
purity. 
In addition, the chapter illustrates the techniques used to estimate
the background composition of the sample and the comparison between
data and predictions used to validate such estimations.

\section{Event selection}
\label{sec:evtsel}

As discussed in Sec.~\ref{sec:topdecay}, top quarks
decay mainly into a $W$ boson and a $b$ quark. The $W$ boson decays
into a lepton and a neutrino or into two quarks. Therefore, the
semi-leptonic decay channel corresponds to an event topology with
exactly one isolated lepton (muon or electron), missing transverse
energy and at least four reconstructed jets.

A preliminary selection of the events is performed during data
acquisition by the single lepton triggers. Events are recorded when at
least one high--\pt{} muon or electron is reconstructed by the trigger
algorithms. 
The combination of \pt{} and isolation requirements is optimized to ensure
a good selection efficiency in different luminosity
conditions.
Therefore, several trigger configurations are
considered, and events satisfying at least one of the trigger
decisions are used in the analyses.
In the \seventev{} dataset, the {\it EF\_e20\_medium}, {\it
  EF\_e22\_medium}, {\it EF\_e22vh\_medium1} and {\it EF\_e45\_medium1}
triggers are used to select \ejets{} events, while the {\it
  EF\_mu18} and {\it EF\_mu18\_medium} triggers are used to select
\mujets{} events. In the \eighttev{} dataset, events selected by either the {\it
  EF\_e24vhi\_medium1} or {\it EF\_e60\_medium1} triggers are used in the
\ejets{} channel, while {\it EF\_mu24i\_tight} and {\it
  EF\_mu36\_tight} triggers are used for \mujets{} events.
The \pt{} and isolation requirements for each trigger are detailed in
Table~\ref{tab:trigthresholds}. 
The tighter requirements are used at high luminosity, so that
the rate of selected events is kept within the allowed bandwidth for
data--taking. The dedicated triggers with high \pt{} and no
isolation requirements, such as {\it EF\_e45\_medium1}, {\it
  EF\_e60\_medium1} and {\it EF\_mu36\_tight}, are used to compensate
for efficiency losses in the identification of high--\pt{} leptons,
due to the isolation requirements.
\begin{table}\centering
  \begin{tabular}{l|c|ccc}
    \toprule
    & \multicolumn{4}{c}{\seventev{}} \\
    \midrule
    channel&\multicolumn{1}{c|}{\mujets{}}&\multicolumn{3}{c}{\ejets{}}\\
    trigger &
\begin{tabular}[c]{@{}c@{}}{\it EF\_mu18}\\{\it EF\_mu18\_medium}\end{tabular}
&{\it EF\_e20\_medium}&
\begin{tabular}[c]{@{}c@{}}{\it EF\_e22\_medium}\\{\it EF\_e22vh\_medium1}\end{tabular} 
&{\it EF\_e45\_medium1}\\
    trigger $\pt{}_{min}$&\multicolumn{1}{c|}{$18\GeV{}$}&$20\GeV{}$&$22\GeV{}$&$45\GeV{}$\\ 
    offline $\pt{}_{min}$
    &\multicolumn{1}{c|}{$20\GeV{}$}&\multicolumn{3}{c}{$25\GeV{}$}\\ 
    \bottomrule
  \end{tabular}
  \begin{tabular}{l|cc|cc}
    \toprule
    &\multicolumn{4}{c}{\eighttev{}} \\
    \midrule
    channel&\multicolumn{2}{c|}{\mujets{}}&\multicolumn{2}{c}{\ejets{}}\\
    trigger &{\it
      EF\_mu24i\_tight}&{\it EF\_mu36\_tight}&{\it EF\_e24vhi\_medium1}&{\it
      EF\_e60\_medium1}\\
    isolation & $\pt{}^{0.2}_{,tracks}/\pt{}_{\mu}<0.12$ &
    -- & $\pt{}^{0.2}_{,tracks}/\et{}_{e}<0.1$ & --\\
    trigger $\pt{}_{min}$&
    $24\GeV{}$&$36\GeV{}$&$24\GeV{}$&$60\GeV{}$\\ 
    offline $\pt{}_{min}$&
    \multicolumn{2}{c|}{$25\GeV{}$}&\multicolumn{2}{c}{$25\GeV{}$}\\ 
    \bottomrule
  \end{tabular}
  \caption{Lepton \pt{} and isolation requirements for the single
    lepton triggers during 2011 and 2012 data-taking.}
  \label{tab:trigthresholds}
\end{table}

The events used in the analysis are required to contain exactly one
lepton reconstructed offline as described in
Sec.~\ref{sec:objects}. The lepton selected offline is required to
match the corresponding lepton at the trigger
level within $\Delta R<0.15$. Minimum \pt{} requirements ensure that the trigger efficiencies
are maximized and nearly constant as a function of the lepton \pt{} for the
selected events. The electron \pt{} requirement is also used to reduce
the amount of multijet background in the sample. In the analysis at
\seventev{}, exactly one electron or muon with $\pt{}>25\GeV{}$ or
$\pt{}>20\GeV{}$, respectively, is required. In the \eighttev{}
analysis, the \pt{} requirement on both electron and muon is $>25\GeV{}$.

In addition to the lepton, a minimum of four jets with $\pt{}>25 \GeV$
are required to be reconstructed in the event. Only jets reconstructed
in the range $|\eta|<2.5$ are considered (see
Sec.~\ref{sec:jets}). The number of jets tagged as $b$--jets is used
to define \ttbar{}--enriched (with at least one $b$--jet) or
background--enriched (without $b$--jets) samples. The measurements at
\seventev{} are performed in a \ttbar{} enriched sample with at least
one $b$--jet. For the measurements at \eighttev{} the \mbox{$b$--jet}
multiplicity is exploited to estimate precisely the background
composition considering three categories of events: without $b$--jets,
with exactly 1 $b$--jet, and with at least 2 $b$--jets. 

In order to suppress the QCD multijet and Z+jets backgrounds,
requirements on the \met{} and the transverse mass of the
leptonically-decaying W boson (\mtw{}) are applied.\footnote{$\mt = \sqrt{2
    p^\ell_{\rm T} \met (1-\cos\Delta\phi)}$, with $p^\ell_{\rm T}$
  being the transverse momentum of the lepton and
  $\Delta\phi$ the azimuthal angle separation between the lepton and
  the direction of the missing transverse momentum.} Since the sample
composition varies with the lepton flavor and $b$--jet multiplicity,
different requirements are applied for each scenario. As detailed in
Table~\ref{tab:metmtwcuts}, in the analysis at \seventev{} tighter
requirements on \met{} and \mtw{} (both $>30 \GeV$) are applied in the
\ejets{} sample, in order to suppress the larger multijet background,
while in the \mujets{} sample the requirements are $\met{}>20\GeV{}$
and $\met{}+\mtw{}>60 \GeV{}$. The latter, referred to as {\it
  triangular cut}, is used for both \ejets{} and \mujets, in the
analysis at \eighttev{}. 
In the measurements at \eighttev{}, a tighter requirement
($\met{}>40\GeV{}$) is applied in the background--enriched sample with
0 $b$--jets, while it is removed in the \ttbar{}--enriched sample,
where the requirement of at least 2 $b$--jets is sufficient to reject
background events. This allows to achieve a suppression of the
\zjets{} background that allows a more precise estimation of the
\wjets{} background (see Sec.~\ref{sec:wjets}). 

\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
     &$\sqrt{s} = 7 \TeV$&\multicolumn{3}{c}{$\sqrt{s} = 8 \TeV$}\\
    channel&$\geq{}1$ $b$-jets&0 $b$-jets&1 $b$-jet&$\geq{}2$ $b$-jets\\
    \midrule
    \multirow{2}*{\mujets{}} & $\met{}>20 \GeV{}$ & $\met{}>40
    \GeV{}$ & $\met{}>20 \GeV{}$ & -- \\
                                           &
                                           $\met{}+\mtw{}>60 \GeV{}$ &
                                           $\met{}+\mtw{}>60 \GeV{}$ &
                                           $\met{}+\mtw{}>60 \GeV{}$ &
                                           -- \\

     \multirow{2}*{\ejets{}} & $\met{}>30 \GeV{}$ & $\met{}>40 \GeV{}$
     & $\met{}>20 \GeV{}$ & -- \\ 
                                         & $\mtw{}>30 \GeV{}$ &
                                       $\met{}+\mtw{}>60 \GeV{}$ &
                                       $\met{}+\mtw{}>60 \GeV{}$ & -- \\
    \bottomrule
  \end{tabular}
  \caption{Minimum \met{} and \mtw{} requirements.}
  \label{tab:metmtwcuts}
\end{table}

\clearpage

\section{Signal and background modeling}
\label{sec:bckgmodel}

The requirements described in Sec.~\ref{sec:evtsel} are designed to
select semi--leptonic \ttbar{} decays. However, fully
leptonic decays can also be selected when one of the two leptons is not
reconstructed. These events are considered as part of the signal
sample, notwithstanding the fact that the \ttbar{} kinematics cannot be
properly reconstructed under the semi-leptonic hypothesis.

Even though the requirements are designed to select the \ttbar{}
topology, other processes contaminate the samples: $W$ and $Z$ boson
production in association with jets (\wjets{}, \zjets{}), single top,
QCD multijet, and diboson ($WW$, $ZZ$, $WZ$) production constitute a
non-negligible fraction of the selected events.
In the case of \wjets{} events, the largest background, the leptonic W
decay produces a high-\pt{} isolated lepton, while the additional jets
production, including $b$-jets, can potentially mimic the hadronic top quark decay.
Analogously the leptonic decays of a vector boson in \zjets{} and diboson
processes feature high-\pt{} leptons reconstructed with high
efficiency.
Finally, while the misidentification of jets as reconstructed leptons is very
rare, the large cross section for QCD multijet production still results in a
non--negligible amount of background events, whose normalization and
kinematic features need to be properly estimated.

The background composition of the data samples is estimated with a
combination of MC
simulations and data--driven techniques. In particular the single top 
and \wjets{} backgrounds are charge asymmetric and need to be estimated
carefully in order to limit the impact on the measurement. The single top
production is estimated with precise simulations at NLO, while the
expected yield of \wjets{} events and the flavor composition of the
associated jets are calibrated in-situ. The shapes of the
distributions and the relative normalization across samples are
predicted with simulation.
The QCD multijet background is fully estimated from data.

\subsection{Simulated samples}
\label{sec:mcsamples}

Different MC generators, PDF sets and parton shower and fragmentation
algorithms are used to model signal and backgrounds in the data
samples collected at \seventev{} and at \eighttev{}, with the goal of
providing the most reliable possible prediction in each scenario.
Developments in the MC algorithms and the improved understanding of
data led, in some cases, to changes in the analysis of the \eighttev{}
dataset, with respect to the choices made for the analysis of the
\seventev{} one. 
In the following the different choices for each process are
listed. The details of each algorithm are given in Sec.~\ref{sec:generators}.

The \ttbar{}+jets process for the analysis of the dataset at
\seventev{} is simulated using the \alpgen{} generator with the {\tt
  CTEQ6L1} PDF set, interfaced to the \herwig{} parton shower.  
At \eighttev{}, the \powheg{} generator is used with the {\tt CT10}
PDF set, and it is interfaced to the \pythia{} parton shower. 
The \powheg{} simulated sample is calibrated to the ATLAS measurement of the
unfolded differential cross section at \seventev{}~\cite{Aad:2014zka}
by reweighing sequentially the \pt{} of the top quark and of the
\ttbar{} system. This calibration improves the description of the
\pt{} spectra of the decay products of the top quark and of the jet
multiplicity spectrum.

Simulated samples of $W/Z$ boson production are generated with up to
five additional partons using the {\tt ALPGEN} generator and the
{\tt CTEQ6L1} PDF set, interfaced to \herwig{} (\seventev{} analysis) or
\pythia{} (\eighttev{} analysis) for parton showering and fragmentation. 
Separate samples are produced to simulate $W$ production in
association with bottom quark pairs (\wbb{}), charm quark
pairs (\wcc{}), single charm quarks (\wc{}), and light
flavor partons, including gluons (\wlight{}). The samples are initially
normalized to the NNLO theoretical cross sections ~\cite{vjetsxs} and
further corrected with data--driven calibration scale factors
(see Sec.~\ref{sec:wjets}).
The $Z$+jets samples are generated separately for $Zb\bar{b}$+jets,
$Zc\bar{c}$+jets and $Z$+light jets and normalized to the inclusive
NNLO theoretical cross section~\cite{vjetsxs}.
The overlap between $W/ZQ\bar{Q}$+jets ($Q=b,c$) events generated from the
matrix element calculation and those generated from parton-shower
evolution in the $W/Z$+light jets samples is avoided by using the
matrix element prediction only for $\Delta R(Q,\bar{Q})>0.4$.

In the \seventev{} analysis, simulated samples of single top quark
production via $s$--channel and $Wt$ processes are generated with
\mcatnlo{} interfaced with the \herwig{} parton shower, while the
$t$--channel production is simulated with the {\tt ACERMC} LO
generator with the {\tt MRST LO**} PDF set.
At \eighttev{} all three processes are simulated with \powheg{}
interfaced with \pythia{}, using the {\tt CT10} PDF set.
Single top samples are normalized to the corresponding NLO cross
sections~\cite{stschan,sttchan,stwt}. 
Finally, diboson production is modelled using \herwig{} with
the {\tt MRST LO**} PDF set for the \seventev{} analysis, while for
the \eighttev{} one \alpgen{} is used, interfaced with \herwig{} for
the parton shower, with the {\tt CTEQ6L1} PDF set. In both cases, the
sample is normalized to the NLO theoretical cross
sections~\cite{dibosonxs}.

\subsection{\wjets{} background normalization}
\label{sec:wjets}

The normalization of the \wjets{} background is measured in data in
order to constrain its uncertainty.
The procedure exploits the difference in production cross section at LHC
between $W^+$ and $W^-$ to estimate the \wjets{} yield.
Due to the higher density in protons of $u$ quarks with respect to $d$ quarks,
the cross sections $\sigma(u\bar{d}\to W^+)$ and $\sigma(d\bar{u}\to
W^-)$ are different, with a larger production rate expected for $W^+$. The
prediction for the $W$ boson charge asymmetry in \wjets{} production is less
affected by theoretical uncertainties~\cite{wasym} and can be
exploited, in combination with measurements of $W^+$ and
$W^-$--enriched data samples, to derive the correct overall normalization for the MC prediction.
The total number of \wjets{} events in the selected data sample
$N_{W}=N_{W^+}+N_{W^-}$ can be estimated as
\begin{equation}
N_{W} = \left(\frac{N_{W^+}+N_{W^-}}{N_{W^+}-N_{W^-}}\right )_{\rm
  MC}(N_{W^+}-N_{W^-})_{\rm meas}
\phantom{,}
\label{eq:nw}
\end{equation}
where positive and negative $W$ bosons are identified from the
charge of the reconstructed lepton. 

The $W$ boson charge asymmetry observed in simulation depends
on the flavor composition of the sample, as the size and sign of the
asymmetry varies for \wbb{}, \wcc{}, \wc{} and \wlight{} production,
as summarized in Table~\ref{tab:wca}.
\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
     & \multicolumn{4}{c}{fraction/lepton charge asymmetry} \\
    process & {\it pretag} & 0 $b$--jets & 1 $b$--jet & at least 2 $b$--jets \\
    \midrule
    \wbb{} & 9\%/0.24 & 3\%/0.28 & 28\%/0.23 & 71\%/0.20 \\
    \wcc{} & 18\%/0.27 & 16\%/0.29 & 25\%/0.23 & 14\%/0.19 \\
    \wc{} & 16\%/-0.07 & 14\%/-0.07 & 27\%/-0.07 & 11\%/-0.08 \\
    \wlight{} & 57\%/0.28 & 67\%/0.28 & 20\%/0.24 & 4\%/0.25 \\ 
    \midrule
    Total \wjets{} & --/0.22 & --/0.23 & --/0.15 & --/0.17 \\
   \bottomrule
  \end{tabular}
  \caption{Composition of the simulated \wjets{} background and lepton charge
    asymmetry of each flavor in the \ljets{} pretag sample.}
  \label{tab:wca}
\end{table}
Therefore a calibration of the flavor composition is derived
simultaneously with the estimation of the total normalization in
Eq.~\ref{eq:nw}.
The relative fractions are estimated in a \wjets{} enriched control
region where exactly two reconstructed jets are required and no
$b$--tagging requirement is applied ({\it pretag} region). 
The additional requirement of at least one $b$--tagged jet is applied
to define a \wbb{} enriched region ({\it tag} region) and the \wjets{}
event yield is given by: 
\begin{equation}
\label{eq:nwt}
N^{W,{\rm tag}} = 
N^{W,{\rm pretag}}
\sum_{x=\bbbar{}, \ccbar{}, c, light} F_xP_x,
\end{equation}
where $F_x$ are the flavor fractions $N^{\rm pretag}_x/N^{\rm pretag}$
and $P_x$ is the selection efficiency of the $b$--tagging requirement for each
flavor type $x = \bbbar{}, \ccbar{}, c, light$. 
 With the assumption that \wbb{} and \wcc{} simulations require a
 fully correlated calibration, given the similarity of the processes,
 three calibration factors $K_{\bbbar{}/\ccbar{}}=F^{\rm
   data}_{\bbbar{}}/F^{\rm MC}_{\bbbar{}}=F^{\rm
   data}_{\ccbar{}}/F^{\rm MC}_{\ccbar{}}$, $K_c=F^{\rm
   data}_{c}/F^{\rm MC}_{c}$ and $K_{light}=F^{\rm
   data}_{light}/F^{\rm MC}_{light}$ are estimated to fit data. The
 calibration factors are then extrapolated to the signal region, where
 at least four jets are required, using the MC prediction
to renormalize to unity the sum of the flavor fractions.
The extrapolation involves additional systematic uncertainties, thus
limiting the precision of the calibration.
Table~\ref{tab:wsf} summarizes the flavor fraction and normalization
calibration factors derived in the \ejets{} and \mujets{} channel for
the dataset at \seventev{}.

\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    dataset & channel &$K_{\bbbar{}/\ccbar{}}$&$K_{c}$&$K_{light}$ \\
    \midrule
   \multirow{2}*{\seventev{}} &\mujets{} & $1.2\pm0.4$& $1.0\pm0.4$ &$0.97\pm0.09$\\ 
                                             &\ejets{}    & $1.4\pm0.4$ & $0.7\pm0.4$ &$1.00\pm0.10$\\ 
    \midrule
    \eighttev{} &\ljets{} & $1.50\pm0.11$& $1.07\pm0.27$&$0.80\pm0.04$\\ 
   \bottomrule
  \end{tabular}
  \caption{Calibration factors for the normalization of the \wjets{}
    flavor components as measured in the datasets at \seventev{} and
    \eighttev{}. The uncertainties include statistical and systematic
    components.}
  \label{tab:wsf}
\end{table}

For the analysis of the dataset at \eighttev{}, a similar approach is used by
performing the {\it in-situ} calibration embedded in the unfolding
procedure described  in Sec.~\ref{sec:unfolding}. The three $b$--jet
multiplicity subsamples are further split according to the lepton charge, thus
obtaining six independent samples. The $b$--jet multiplicity provides
information about the heavy-flavor composition of the \wjets{}
background, while the lepton charge is used to determine the
normalization of each component. Data and predictions for the six
channels used for the inclusive \ac{} measurement are compared in
Fig.~\ref{fig:datamc_prefit}. The two background enriched subsamples
without $b$--jets provide little information about the \ttbar{}
asymmetry, therefore no shape information is used.
The three calibration factors for \wbb{}/\wcc{}, \wc{} and \wlight{}
obtained from the inclusive \ac{} measurements are reported in
Table~\ref{tab:wsf}. It has been checked that these factors are
consistent between \ejets{} and \mujets{} channels (see
App.~\ref{app:unf:evsmu}). 
These factors are not expected to be consistent
with the ones shown in Table~\ref{tab:wsf}, since they represent
calibrations of different simulation chains (\alpgen{}+\herwig{} is
used at \seventev{}, while \alpgen{}+\pythia{} is used at
\eighttev{}). The improved precision is due to the use of the three
$b$--jet multiplicity samples (0, 1, at least 2 $b$--jets) and the
more precise $b$--tagging calibration. 

\begin{figure}\centering
  \includegraphics[width=0.75\textwidth]{figures/selection/datamcinclu8TeV_prefit}
  \caption{
   Comparison between data and predictions in the six channels
   considered for the measurement at \eighttev{}. The plot shows the
   distributions used to measure the inclusive \ac{}. In the samples
   without $b$--jets, the overall yield is compared, while the four bins
   of the \dy{} distribution are shown for each of the
   \ttbar{}--enriched samples with 1 or at least 2 $b$--jets. The
   normalization uncertainty on backgrounds other than \wjets{} is shown. 
  }
  \label{fig:datamc_prefit}
\end{figure}

\subsection{Multijet background}
\label{sec:qcdbckg}

Multijet events can pass the selection criteria when a lepton is
reconstructed in the event. The dominant sources of spurious leptons
are:
\begin{itemize}
\item semi--leptonic $b$-jet decays;
\item long--lived weakly decaying states such as $\pi^{\pm}$ or $K$
  mesons;
\item reconstruction of $\pi^0$ showers as electrons;
\item reconstruction of electrons from conversions or direct photons.
\end{itemize}

While the probability of a QCD multijet event being selected is very
low, the production cross section for multijet events is orders of
magnitude above that of \ttbar{} production.
Because of this and the fact that this probability depends on the
detector configuration and geometry, it is more efficient to
determine the multijet background in the selected sample from data
with a technique referred to as {\it matrix method}~\cite{matrixmethod}.

The matrix method is based on the selection of two categories of
events: events that satisfy loose lepton selection requirements, and
events that satisfy tight lepton selection requirements. The tight
requirements are the ones used in the analysis (see
Sec.~\ref{sec:electrons} and Sec.~\ref{sec:muons}), while the loose
selection is obtained by applying looser isolation and identification
criteria. For the loose muon selection the mini--isolation requirement
is dropped, while loose electrons are selected with the
\texttt{medium++} identification and without applying additional
isolation requirements.
Therefore, the tight lepton sample constitutes a subset of
the loose lepton one.
The number of events with one loose lepton and
the number of events with one tight lepton can be written as:
%                                                                                             
\begin{eqnarray}
  N^\mathrm{loose}
  & = & N^\mathrm{loose}_\mathrm{real}
  + N^{\mathrm{loose}}_\mathrm{fake} \nonumber \\
  N^\mathrm{tight}
  & = & N^\mathrm{tight}_\mathrm{real}
  + N^{\mathrm{tight}}_\mathrm{fake} =
  \effReal{}N^\mathrm{loose}_\mathrm{real} + \effFake{}N^\mathrm{loose}_\mathrm{fake},
  \label{eqn:intro-mm-Nloose}
\end{eqnarray}
%
where $N^\mathrm{loose}_\mathrm{real}$ and
$N^\mathrm{tight}_\mathrm{real}$ are the number of events with a real
lepton satisfying loose and tight lepton requirements, respectively,
and $N^\mathrm{loose}_\mathrm{fake}$ and
$N^\mathrm{tight}_\mathrm{fake}$ are the number of events without a ``real''
lepton but still satisfying loose and tight lepton requirements, respectively.
The loose lepton selection is defined so that the marginal efficiency
$\epsilon=N^\mathrm{tight}/N^\mathrm{loose}$ is sufficiently different between real
leptons and fake leptons. The real--lepton efficiency
%                                                                                             
\begin{equation}
  \effReal{}
  = \frac{N^\mathrm{tight}_\mathrm{real}}{N^\mathrm{loose}_\mathrm{real}}
\end{equation}
is measured in data events with $Z$ boson decays in two leptons.
The fake--lepton efficiency
\begin{equation}
  \effFake{}
  = \frac{N^\mathrm{tight}_\mathrm{fake}}{N^\mathrm{loose}_\mathrm{fake}}
  \label{eqn:intro-mm-real-fake}
\end{equation}
is measured in data control regions with small \met{} and \mtw{},
where the contribution from fake leptons is larger. While \effReal{}
is above 0.8 for both electron and muons and does not depend on the
event topology, \effFake{} is parametrized as a function of kinematic
observables such as lepton $\eta$ and \deltaR{\ell{}}{jet}. Typical
values for \effFake{} lie between 0.2 and 0.4 for electrons and
between 0.1 and 0.3 for muons. With the two efficiencies known, a
system of two linear equations (\ref{eqn:intro-mm-Nloose}) can be
solved for $N^{\mathrm{tight}}_\mathrm{fake}$ as a function of
$N^\mathrm{loose}$ and $N^\mathrm{tight}$, obtaining:
%                                                                                             
\begin{equation}
  N^\mathrm{tight}_\mathrm{fake}
  = \frac{\effFake{}}
  {\effReal{} - \effFake{}}
  (N^\mathrm{loose} \effReal{} - N^\mathrm{tight}).
  \label{eqn:intro-mm-tight_fake}
\end{equation}
%                                                                                             
Therefore, the QCD multijet prediction is estimated by weighing the
selected data events:
events satisfying the tight lepton requirements are assigned a weight
equal to $\effFake{}(\effReal{}-1)/(\effReal{}-\effFake{})$, while
events satisfying the loose lepton requirements, but not the tight
requirements, are assigned a weight equal to
$(\effFake{}\cdot{}\effReal{})/(\effReal{}-\effFake{})$. This allows
the prediction of not only the normalization, but also the shape of
kinematic distributions for the QCD multijet background.

\section{Comparison between data and prediction}
\label{sec:datamc}

The expected and observed number of selected events in both the
\mujets{} and the \ejets{} channels are reported in
Table~\ref{tab:yields2011} for the dataset at \seventev{}, and in
Tables~\ref{tab:yields2012mu} and \ref{tab:yields2012ele}  for \eighttev{}.

\input{tex/selection_tables.tex}

The modeling of the main background process is validated by comparing
distributions as observed in the data samples with the corresponding
predictions from simulation. 
Before applying $b$--tagging requirements, the sample composition can
be studied without biases toward a specific flavor composition. As
shown in Tables~\ref{tab:yields2011}--\ref{tab:yields2012ele}, 
the purity of the \ttbar{} signal is about
50\%, while the largest background comes from the \wjets{}
process. Fig.~\ref{fig:pretagdatamc} shows that the various
backgrounds populate different kinematic regions depending on their
properties. For processes with real $W$ bosons, such as top quark(s)
and \wjets{} production, the \mtw{} distribution peaks slightly below
the mass of the $W$ boson, while processes without neutrinos, such as
$Z(\to\ell\ell)+$jets and the multijet background, are more prominent
at low \met{} values.
 
\begin{figure}\centering
  \includegraphics[width=0.495\textwidth]{figures/selection/7TeV/mujets/pretag/WMt}
  \includegraphics[width=0.495\textwidth]{figures/selection/8TeV/ejets/pretag/met}
  \caption{
    Comparison between data and prediction in the pretag sample: the
    \mtw{} distribution in the \mujets{} channel at \seventev{}
    (left), and the \met{} distribution in the \ejets{} channel at
    \eighttev{} (right). The normalization uncertainties are shown.}
  \label{fig:pretagdatamc}
\end{figure}

The fraction of \ttbar{} events can be enhanced by requiring one or
at least two $b$--tagged jets, and the pure samples are used to check the
modeling of the kinematic quantities of the reconstructed
objects used for selection and reconstruction, as shown in
Fig.~\ref{fig:taggeddatamc}. Events with at least 1 $b$--jet are
eventually used to measure the asymmetry. A total of about 50000 and
320000 \ttbar{} events are selected in the dataset at \seventev{} and
\eighttev{}, respectively.
The distributions in data of the lepton \pt{} and $\eta$, the
leading\footnote{Jets are conventionally ordered in decreasing \pt{};
  hence the ``leading jet'' is the reconstructed jet with the highest
  \pt{}.} jet \pt{} and $\eta$ are in good agreement with the
predictions. Comparisons for analogous distributions in background-- and
signal--enriched samples are shown in App.~\ref{app:selection}.

\begin{figure}\centering
  \subfloat[][]
{
  \includegraphics[width=0.45\textwidth]{figures/selection/8TeV/mujets/oneTag/lep_pt_1}
}\quad
  \subfloat[][]
{
  \includegraphics[width=0.45\textwidth]{figures/selection/8TeV/ejets/oneTag/lep_eta_1}
}\\
  \subfloat[][]
{
  \includegraphics[width=0.45\textwidth]{figures/selection/8TeV/mujets/twoTag/jet_pt_1}
}\quad
  \subfloat[][]
{
  \includegraphics[width=0.45\textwidth]{figures/selection/8TeV/ejets/twoTag/jet_eta_1}
}
  \caption{
    Comparison between data and prediction in the \eighttev{} sample:
    distributions of (a) the lepton \pt{} in the \mujets{} channel with
    exactly 1 $b$--jet, (b) the lepton pseudo--rapidity in the \ejets{}
    channel with exactly 1 $b$--jet, (c) the leading jet \pt{} in the
    \mujets{} channel with at least 2 $b$--jets and (d) the leading
    jet pseudo--rapidity in the \ejets{} channel with exactly 2
    $b$--jets.
}
  \label{fig:taggeddatamc}
\end{figure}

\section{Systematic uncertainties}
\label{sec:syst}

The precision of the measurements depends on two aspects: the
amount of signal events in the sample, determining the statistical
component of the uncertainty, and the systematic uncertainties. 
Sources of systematic uncertainties are the finite precision of the
calibration of the reconstructed objects, the imperfections of signal
and background modeling and the limited description of the
experimental conditions (e.g. luminosity, pile-up).
Systematic uncertainties affect both the normalization of the total event yield and the
shape of the kinematic distributions.

The individual sources of systematic uncertainty are treated as
uncorrelated from each other, while the effect of each source is fully
correlated across processes and analysis channels.
The sources of systematic uncertainties considered in this work are
discussed in the following.

\subsection{Luminosity}
\label{sec:syst_lumi}
The uncertainty on the integrated luminosity is estimated to be
of 1.8\% at \seventev{}~\cite{lumiunc} and 2.8\% at \eighttev{}. This
systematic uncertainty affects all processes for which the event yield
from simulation is used, \zjets{}, diboson, single top.

\subsection{Object definitions}
\label{sec:syst_objects}
The object reconstruction and calibration introduces uncertainties
associated with the definition of leptons, jets, \met{} and on the jet
flavor-tagging. In the following the corresponding systematic
uncertainties are discussed. 

\subsubsection{Lepton reconstruction, identification and trigger efficiencies}
\label{sec:syst_lepID}

The efficiencies of triggering, reconstructing and identifying leptons
(see Sec.~\ref{sec:electrons} and~\ref{sec:muons})
are calibrated in the simulation with scale factors to match the ones
in data. The calibrations are derived with tag-and-probe techniques in
$Z\to \ell^+\ell^-$ ($\ell=e,\mu$) data and simulated samples. The
associated systematic uncertainty is taken into account by
comparing the simulated samples with scale factors varied within their
uncertainties.

In the \ejets{} channel, the overall effects on predicted yields are 0.3\%, 1.1\% 
and 0.2\% at \seventev{} and 0.3\%, 2.4\% and 0.7\% at \eighttev{},
respectively for electron reconstruction, identification and trigger
efficiency uncertainties.
In the \mujets{} channel, the effects are 0.2\%, 1.1\% and 1.4\% at
\seventev{} and 0.4\%, 0.5\% and 1.8\% at \eighttev{},
respectively for muon reconstruction, identification and trigger
efficiency uncertainties.

\subsubsection{Lepton momentum scale and resolution}
\label{sec:lepscale}

The lepton momentum scale and resolution is calibrated using
simulated samples of $Z\to \ell^+\ell^-$ and $J/\psi \to
\ell^+\ell^-$.
The calibration for muons is applied by adjusting the muon momentum
scale and resolution in the simulation to match data. In the case of
electrons, the energy resolution is smeared in simulated samples to
match data, while the energy scale corrections are applied to data in
all detector regions and to simulation only in the calorimeter
transition region. 
The systematic uncertainties associated with these calibrations vary
slightly the selection acceptance, with effects on the event yield
below the percent level. Examples of the calibration measurements are
shown in Fig.~\ref{fig:lepscalesyst}.

\begin{figure}\centering
  \includegraphics[width=0.495\textwidth]{figures/selection/systs/muonmomentumscale}
  \includegraphics[width=0.495\textwidth]{figures/selection/systs/eleenergyscale}
  \caption{
    Measurements of the lepton momentum scale calibration for muons
    (left)~\cite{Sforza:1529999} and electrons
    (right)~\cite{Aad:2014nim}. The shaded regions represent the
    systematic uncertainties. 
  }
  \label{fig:lepscalesyst}
\end{figure}

\subsubsection{JVF efficiency}
\label{sec:syst_jvf}

The efficiency of the JVF requirement (Sec.~\ref{sec:jets}) is
calibrated in simulation using scale factors derived in $Z(\to
\ell^+\ell^-)$+1-jet events. In the analysis at \seventev{}, the scale
factor for pileup jets efficiency is constant and $\sim1$, while the
hard--scatter jets efficiency is scaled up by a variable amount
between $\approx3\%$ for jets with $\pt{}=25\GeV{}$ and down to $1\%$
for jets with $\pt{}>150\GeV{}$. The uncertainties on the scale
factors is propagated to the predicted event yields with effects of
$\sim2.5\%$. In the analysis at \eighttev{}, no scale factors are
applied to the simulation, and the systematic
uncertainty is estimated by varying the requirement within the
efficiency range corresponding to the discrepancy between data and
simulation. The overall effect on event yields is of about 2.5\%.

\subsubsection{Jet energy scale}
\label{sec:syst_jes}

The Jet Energy Scale (JES) is derived from measurements at test-beam 
and measurements using data and simulation~\cite{jes}. 
The associated \pt{} and $\eta$--dependent uncertainty is propagated
to the predicted distributions by varying up and down the energy of
all reconstructed jets of each event. For each variation, the jet
four--momenta and the missing transverse momentum
\met{} are recomputed consistently to the varied $\pt$ of the jets.

Pile-up activity introduces an additional source of systematic 
uncertainty that depends on the number of primary vertices
and on the average number of interactions per bunch crossing $<\mu>$. 
Momentum balance techniques in $Z$+jets, $\gamma$+jets and 
multi-jet events are combined to derive a small residual correction
for jets in the transverse momentum range $20\GeV{}<\pt{}\lesssim
1\TeV$ to take account of this effect.

The overall variation due to JES systematic uncertainty 
evaluated in the central detector region 
is $\sim$4\% for jets with $\pt{}=25\GeV$ and improves to $\sim$1\% for  
jets with $\pt=500\GeV$, as shown in Fig.~\ref{fig:jessyst}.

\begin{figure}\centering
  \includegraphics[width=0.495\textwidth]{figures/selection/systs/jessystpt}
  \includegraphics[width=0.495\textwidth]{figures/selection/systs/jessysteta}
  \caption{
    Different components of the JES uncertainty as a function of \pt{}
    (left) and $\eta$ (right).
  }
  \label{fig:jessyst}
\end{figure}

\subsubsection{Jet energy resolution}
\label{sec:syst_jer}

The Jet Energy Resolution (JER) is measured in data and simulation
as function of the jet transverse momentum and pseudo--rapidity.
The measurement shows compatible resolutions in data and simulation,
therefore no correction is applied. However, the quadratic difference
between the JER in data and in simulated samples is used to smear the
energy of jets in the simulation. The symmetrized templates with
respect to the nominal case are used as positive and negative
variations.

\subsubsection{Flavor tagging}
\label{sec:syst_btag}

The efficiencies of flavor ($b$, $c$, or light) jets identification with
the $b$-tagging algorithm (Sec.~\ref{sec:btag}) are measured in data
for each flavor~\cite{btagging,ctagging,ltagging}.
In simulation $b$ ($c$) jet efficiencies are calibrated with
\pt{} and $\eta$--dependent scale factors in the range 0.9--1.0 (1.1--1.2), the
scale factor for light tagging efficiency is $\sim$1.3.
The uncertainty on the scale factors for the analysis at \seventev{}
are  between 7\% and 13\% for $b$ jets, between 15\% and 39\% for $c$
jets, and $\sim$25\% for light jets. At \eighttev{} the precision is
improved with uncertainties below 3\% for $b$-- and $c$--jet scale
factors and of about 5\% for light jets.

The systematic uncertainty on flavor tagging efficiency is divided
into six independent components corresponding to the \pt{} bins used for
the efficiency measurement. Therefore a total of 18 uncorrelated
systematic uncertainties -- six per flavor -- is considered.
For each component, a per-jet weighting procedure~\cite{IFAEBtagNote}
is applied to simulated events in order to propagate the calibration
uncertainties.

\subsection{Background normalizations}

\subsubsection{Cross sections}
\label{sec:syst_bkgxsect}

 The single top, \zjets{} and diboson processes constitute a small
fraction of the background, which is estimated using simulation only.
Therefore the corresponding normalization uncertainty is determined by
the precision of the associated theoretical cross section.
Uncertainties of $\pm4.7\%$, $\pm4\%$ and $\pm 5\%$ are assumed
for the theoretical cross sections of the single
top~\cite{stschan,sttchan,stwt}, \zjets{} and diboson~\cite{dibosonxs}
backgrounds respectively.
In addition, a $48\%$ uncertainty~\cite{berends} is associated to the
\zjets{} and diboson normalizations, accounting for the poor modeling
of the jet multiplicity in LO generators.

\subsubsection{\wjets{} calibration}
\label{sec:syst_wjets}

For the \seventev{} measurements, the overall \wjets{} normalization
uncertainty results from the calibration procedure described in
Sec.~\ref{sec:wjets}. The total uncertainty on the \wjets{} yield in
the signal region is about $40\%$ for \ejets{} channel and $20\%$ in
\mujets{}. However the correlation with other sources of uncertainties
is properly handled by applying the corresponding normalization and
heavy--flavor scale factor for each source of uncertainty considered.
Such scale factors are determined by applying the calibration
procedure using the varied templates for each sample.

For the \eighttev{} measurements, the calibration of the \wjets{}
background is embedded in the unfolding procedure; therefore no
a--priori uncertainty is assigned. The overall normalization
uncertainties on \wbb{}/\wcc{}, \wc{} and \wlight, obtained with the
combined \ljets{} measurement  are about $10\%$, $25\%$ and $5\%$,
respectively.

\paragraph{PDF}
\label{sec:syst_pdf}

The charge asymmetry in $W$ boson production, on which the calibration
of the \wjets{} background relies, is due to the different density of
positive and negative incoming partons. Therefore the modeling of the
PDFs in the simulated \wjets{} samples is relevant. In order to check
the impact of the choice of the PDFs parametrization and its
uncertainty, the measurements are repeated using \wjets{} simulations
with variations of the CT10, MSTW, and NNPDF PDFs
sets~\cite{pdf4lhc}. A small effect ($\sim0.001$) is seen in the
inclusive \ac{} measurements, while the effect is negligible for the
differential measurements at \seventev{} and small (10--20\% of
statistical error) at \eighttev{}.

\subsubsection{Multijet}
\label{sec:syst_qcd}

Systematics uncertainties affecting the multijet background originate
from the difference between estimates obtained using different control
regions and from the calibration of the method using simulated
multijets events. For the \seventev{} measurements, a $50\%$ ($20\%$)
normalization uncertainty is assigned for the \ejets{} (\mujets{})
channel.
In the \eighttev{} measurements a $50\%$ uncorrelated uncertainty is assigned to the
normalization of the multijet templates for each of the $b$-tagging
multiplicities considered.

\subsection{Signal modeling}

The signal simulation is used to perform the unfolding procedure
described in Sec.~\ref{sec:unfolding}. In order to investigate the
impact of the \ttbar{} signal modeling, additional samples generated
with \powheg{} interfaced with \herwig{} and \mcatnlo{}
interfaced with \herwig{} are considered. 